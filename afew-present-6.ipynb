{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Fisheries Economics workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical bio-economic modelling\n",
    "\n",
    "\n",
    "**Aim**:  Estimate the unknown parameters in the Gordon-Schaefer fisheries model using data from the fishery\n",
    "\n",
    "What we need:\n",
    "\n",
    "- Data (There is lots of data but data infrastrcuture and data governance is not well developed in the Pacific. Lack of data is not the problem) In recent work we have been working with over 970,000 observations so we are approaching data sets of 1 million observations in some of our work.\n",
    "\n",
    "- A way to manipulate data (Pandas)\n",
    "\n",
    "- A way to estimate paprameters (Statsmodels)\n",
    "\n",
    "- A way to extract the policy from the parameter estimates or relate the policy to the parameter estimates\n",
    "\n",
    "This is an example of structural econometrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where do we get data?\n",
    "\n",
    "\n",
    "- Market and trade data: Company websites (thai Union) and government trade statistics (EU, Japan, Thailand, US)\n",
    "\n",
    "- Data aggregators [Quandl](https://www.quandl.com/) has an API and [directly accessible using Python](https://www.quandl.com/tools/python)\n",
    "\n",
    "- Central Banks US Federal Reserve [FRED](https://fred.stlouisfed.org/) database ([API](https://github.com/mortada/fredapi)), others by downloading\n",
    "\n",
    "- SPC (usually sent via Slack in spreadsheet form very inefficient\n",
    "\n",
    "- Members (usually e-mailed in a spreadsheet also very inefficient) \n",
    "\n",
    "- API (Application programming interface) World Bank, UN Comtrade, Quandl\n",
    "\n",
    "- Webscraping company websites - Also a possible workshop topic\n",
    "\n",
    "- Surveys (last reort) - This might be a topic of a futture workshop on survey sampling methods and design\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where do we put data?\n",
    "\n",
    "\n",
    "- relational database (Postgresql) in the cloud (Safe and secure)\n",
    "\n",
    "- spreadsheets (old way of doing it): Unsafe and difficult to manage\n",
    "\n",
    "- How do we get data from a database: **E**xtract **T**ransform **L**oad  (ETL) tool.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility\n",
    "\n",
    "\n",
    "- Using API's enhances reproducibility (automation of data collection helps)\n",
    "\n",
    "- Webscraping enhances reproducibility\n",
    "\n",
    "- Reproducibility means we want to be able to reproduce graphs and results in any reports in such a way that they can be traced back to the original data. Think of it as traceability for data and report writing. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install psycopg2\n",
    "import psycopg2 #need this to connect to the database you will need to install this from the command line with pip install psycopg2\n",
    "# This is why you need administrator rights on your computer. Alternatively load the notebook to Google colaboratory and install \n",
    "#it there using !pip install psycopg2. You have limited administrator rights on Google colaboratory.\n",
    "import sqlite3\n",
    "import io\n",
    "from sqlalchemy import create_engine\n",
    "#import gspread # To link to google sheets only needed for google colaboratory\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PANDAS\n",
    "\n",
    "Make sure you do the tutorial 10 minutes to Pandas\n",
    "\n",
    "[10 Minutes to Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) (Do this as an **exercise!**)\n",
    "\n",
    "Also it's worth reading the [Intro to Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html#getting-started)\n",
    "\n",
    "We have already used Pandas a little above df is a Pandas data frame.\n",
    "\n",
    "We can also import data from a spreadsheet directly into Pandas. This is covered in section 2 of Intro to Pandas.\n",
    "\n",
    "\n",
    "A good reference for [SQL](https://www.w3schools.com/sql/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to a database\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(\"host=  dbname= user= password= \") establishes a connection\n",
    "\n",
    "You need to enter the details that you will be provided with for host, dbname and password\n",
    "\n",
    "cur = conn.cursor() \n",
    "\n",
    "\n",
    "Note this can in theory also be done using Excel Powerquery but may be blocked by your IT department.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host=  dbname= user= password= \") \n",
    "cur = conn.cursor() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries\n",
    "\n",
    "\n",
    "- Now we write a query in SQL (Structured Query Language - pronounced sequel by purists)\n",
    "\n",
    "- We execute the query cur.execute\n",
    "\n",
    "- We fetch the data cur.fetchall()\n",
    "\n",
    "- we pass the data to a data frame (a list of vectors or columns)\n",
    "\n",
    "- Finally check you have the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('SELECT * FROM ace;') # write SQL query in brackets\n",
    "data = cur.fetchall()\n",
    "df = pd.DataFrame(data) # Need to add headers after this call. So need to document this in Manual\n",
    "df #This is a data frame you can writexl to write to Excel file\n",
    "\n",
    "#close the connection after you ahve obtained the data\n",
    "\n",
    "cur.close() \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning with Pandas\n",
    "\n",
    "\n",
    "- Extract the data you need to fit catch to effort (days)\n",
    "\n",
    "- do this by selecting the two columns you need from the data frame df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "\n",
    "First plot your data. Use a scatter plot for this. Catch on the vertical axis and effort on the horizontal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Econometrics\n",
    "\n",
    "We will use the equations from the previous notebook afew-present-5.ipynb\n",
    "\n",
    "- we wish to estimate the relationship between catch and effort the sustainable yield curve from the alst notebook\n",
    "\n",
    "- For the Gordon-Schaefer model that is a quadratic relationship\n",
    "\n",
    "$$ y_i = \\beta_0 +\\beta_{i} X_{i1} + \\beta_2 X_{i}^2 + \\epsilon_i, i=1,\\dots,n$$\n",
    "\n",
    "The theory tells us that $\\beta_0 = 0$.\n",
    "\n",
    "where $X_{i}$ is total fishing effort.\n",
    "\n",
    "Now we ccould just estimate this relationship from the data. However, the theory tells us that $E_{MEY}$ depdns on the model parameters \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the model\n",
    "\n",
    "\n",
    "The model can be estimated using ordinary least squares (linear regression/line of best fit).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm #Import statsmodels for econometrics\n",
    "\n",
    "# need to define y and X you should already have extracted the data and plotted it\n",
    "# so you just need to pass catch to y and days and day squared to X\n",
    "\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit(cov_type='HAC',cov_kwds={'maxlags':1})\n",
    "#print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard errors\n",
    "\n",
    "\n",
    "- Econometric practice has changed over time \n",
    "\n",
    "- \"Credibility revolution\"\n",
    "\n",
    "- Use of asymptotic rather than small sample standard errors\n",
    "\n",
    "- [Newey-West robust standard errors](https://en.wikipedia.org/wiki/Newey%E2%80%93West_estimator) \n",
    "\n",
    "- use of robustness tests\n",
    "\n",
    "- Reduced form is associated with the credibility revolution\n",
    "\n",
    "- Here we are doing a very simple structural model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pd.DataFrame(results.params)\n",
    "hh = params[:1]\n",
    "hsq = params[1:]\n",
    "b0 = hh[0][0]\n",
    "b1 = - hsq[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating MEY from the estimated relationship\n",
    "To determine maximum economic yield we solve the following problem\n",
    "\n",
    "$$\\max \\Pi (E) = p h(E)- cE $$\n",
    "subject to\n",
    "\n",
    "$$\\frac{dX}{dt} = rX(1-\\frac{X}{K})- h(E)= 0$$\n",
    "and $Y(E) = qXE$.\n",
    "\n",
    "In stready state (if you are fishing sustainably) the constraint can be rearranged to obtain\n",
    "\n",
    "$$X = K (1 - \\frac{q}{r} E) $$\n",
    "Substituting into the total rent results in\n",
    "\n",
    "$$ \\Pi (E) = p q K (1 - \\frac{q}{r} E) E - c E = p q K E - \\frac{p q^2 K}{r} E^2  - c E $$\n",
    "Maximizing and solving for E results in\n",
    "\n",
    "$$ E = \\frac{r(pqK - c)}{2 pq^2 K} $$\n",
    "which is exactly half of open access effort level. To determine this level we need the price $p$.\n",
    "\n",
    "Note that sustainable harvest is given by\n",
    "\n",
    "$$ q K (1 - \\frac{q}{r} E) E  =  q K E - \\frac{q^2 K}{r } E^2$$\n",
    "\n",
    "Setting $ q K = b_0 $ and $ \\frac{q^2 K}{r} = b_1$. Wecan then fit a quadratic regression but this implies a constraint on the parameters that depends on the parameter values and so would have to be estimated with Bayesian methods. Alternatively, we can use non-linear least-squares to fit the model.\n",
    "\n",
    "We can rewrite $E_{MEY}$ as\n",
    "\n",
    "$$ \\frac{r(p b_0  - c)}{2 p r b_1} = \\frac{p b_0 - c}{2 p b_1}$$\n",
    "\n",
    " \n",
    "How much will be caught at this level of fishing?\n",
    "\n",
    "$$Y = q E X$$\n",
    "\n",
    "where $X = K (1 - \\frac{q}{r} E)$\n",
    "\n",
    "So\n",
    "\n",
    "$$Y_{TAC} = q E K (1 - \\frac{q}{r} E) = qK E - \\frac{q^2 K}{r} E^2 = b_0 E -b_1 E^2$$$$ =  b_0 \\frac{p b_0 - c}{2 p b_1} -b_1 (\\frac{p b_0 - c}{2 p b_1})^2 $$$$ = \\frac{p b_0^2 - c b_0}{2 p b_1} - \\frac{(p b_0 - c)^2}{4 p^2 b_1}$$\n",
    "\n",
    "We can now compute this and compare with the historical TAC.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to calculate costs\n",
    "\n",
    "\n",
    "- Costs often tobtained from vessel surveys but this requires careful planning to avoid collecting biased data\n",
    "\n",
    "- Alternatively costs can be calculated from catch, the price of fuel/oil and fuel utilization per mt of catch and information on what proportion of the total costs are due to fuel use\n",
    "\n",
    "- This can be validated against other data in the region.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surveys\n",
    "\n",
    "\n",
    "- First identify a sampling frame (A list of all member sof the population, i.e. **a list of all vessels**)\n",
    "\n",
    "- Radomly select a sample from this population\n",
    "\n",
    "- How many to sample?\n",
    "\n",
    "We are interested in the mean cost of fishing per day. We will assume the cost of fishing is normally distributed for purposes of illustration and that we are examining a single gear type.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if $\\bar{x}$ is the mean cost of fishing the standard error of the cost of fishing is given by $\\frac{\\sigma}{\\sqrt{n}}$\n",
    "\n",
    "The sampling bias is given by $\\mu - \\bar{x}$\n",
    "\n",
    "The confidence interval for $\\mu$ is\n",
    "\n",
    "$$\\bar{x} - Z_{\\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{x} + Z_{\\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{n}} $$\n",
    "\n",
    "If we set an error tolerance for the sampling error so that\n",
    "\n",
    "$$ \\mu -\\bar{x} \\leq \\epsilon$$\n",
    "\n",
    "Then $$Z_{\\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{n}} \\leq \\epsilon$$\n",
    "\n",
    "From this we obtain that $n > Z_{\\frac{\\alpha}{2}}^2\\frac{\\sigma^2}{\\epsilon^2}$.\n",
    "\n",
    "Usually we set $\\alpha = 0.05$ for a 9\n",
    "%% confidence interval, $\\sigma$ we otain from previous studies, e.g. past surevys. $\\epsilon$ we choose what error we are prepared to accespt and $Z$ we obtain from the normal distribution tables or from the computer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMEY\n",
    "print('MEY:',EMEY)\n",
    "EMSY = b0/(2*b1)\n",
    "p = 1500 # price \n",
    "c = 10000 # costs per day \n",
    "\n",
    "yTAC = ((p(b0**2) - c*b0)/(2*p*b1)) - ((p*b0 - c)**2)/(4*(p**2)*b1)\n",
    "Ymsy = 0.25*(b0**2)/b1\n",
    "rent = p*(b0*EMEY - b1*EMEY**2) - c*EMEY\n",
    "#M = (r*(p*q*K+c)**2)/(4*K*(p*q)**2) need to express n terms of b0 and b1 can use this to compare to SAM18 M=0.4\n",
    "print('MSY:',EMSY)\n",
    "print('Difference:',EMSY-EMEY)\n",
    "print('TAC',yTAC)\n",
    "print('TAC at MSY',Ymsy) \n",
    "print('Rent',rent)\n",
    "#print('Natural mortality at MEY',M\n",
    "bb1 = yTAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity analysis (Uncertainty quantification)\n",
    "\n",
    "To conduct the [sensitivity analysis](https://en.wikipedia.org/wiki/Sensitivity_analysis) we will use the Sobol sensitivity analysis method, this differs somewhat from a ceteris-paribus approach to sensitivity analysis which is a local method. Sobol or [variance-based sensitivity analysis](https://en.wikipedia.org/wiki/Variance-based_sensitivity_analysis) is a global method, So that it allows us to systematically examine variations in multiple parameters, e.g. price and cost but also the biologial parameters within given confidence bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.sample import saltelli\n",
    "from SALib.analyze import sobol \n",
    "#from SALib.test_functions import Ishigami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gordonschaefer(par):\n",
    "    hh = params[:1]\n",
    "    hsq = params[1:]\n",
    "    b0 = hh[0][0]\n",
    "    b1 = - hsq[0][0]\n",
    "    #p = (155455707/46620575)*1000 # Albacore price per tonne 2018 Thailand average import price\n",
    "    #p = X[0]\n",
    "    \n",
    "    p = param_values[i][0]\n",
    "    E = np.linspace(0,500000,10000)\n",
    "    y_pred = b0*E - (E**2)*b1\n",
    "    rev = p*y\n",
    "    rev_pred = p*y_pred\n",
    "    c = 1.01 # cost per unit effort \n",
    "    \n",
    "    #c = x[1]\n",
    "    #c = param_values[i][1]\n",
    "    \n",
    "    cost = c*E\n",
    "    EMEY = (p * b0 - c)/(2*p*b1)\n",
    "    EMSY = b0/(2*b1)\n",
    "    yTAC = ((p*(b0**2) - c*b0)/(2*p*b1)) - ((p*b0 - c)**2)/(4*(p**2)*b1)\n",
    "    Ymsy = 0.25*(b0**2)/b1\n",
    "    rent = p*(b0*EMEY - b1*EMEY**2) - c*EMEY\n",
    "    \n",
    "    #avghooksperday = df.loc[df['yy']==2017]['hooks per day'].mean() # actual hooks per day.\n",
    "    #vesseldays = EMEY/avghooksperday\n",
    "    #actualdays = df.loc[df['yy']==2017]['days'].sum()\n",
    "    #averagedays = actualdays/n2017\n",
    "    #novessels = vesseldays/averagedays # one could also calculate this by \n",
    "    # flag rather than a broad average, this would probably smooth negotiations\n",
    "    return(EMEY)\n",
    "\n",
    "    # need to take a statsmodels object as an instance and transorm parameter estimates within this function\n",
    "    # Model output might be choosable, examples include vessel numbers, hooks, rent, etc.\n",
    "    \n",
    "\n",
    "\n",
    "# Define the problem\n",
    "problem = {\n",
    "    'num_vars': 2,\n",
    "    'names': ['p', 'c'],\n",
    "    'bounds': [[2000, 5000],\n",
    "               [0.0, 200.0]]\n",
    "}\n",
    "\n",
    "param_values = saltelli.sample(problem, N = 1000)\n",
    "\n",
    "Y = np.zeros([param_values.shape[0]])\n",
    "\n",
    "for i, X in enumerate(param_values):\n",
    "    #print(i,X)\n",
    "    Y[i] = gordonschaefer(X)\n",
    "    \n",
    "\n",
    "#print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = sobol.analyze(problem, Y, calc_second_order = False,print_to_console=True) #this is producing an error\n",
    "print(si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y)\n",
    "plt.title('Histogram of vessel numbers using Sobol sensitivity analysis 1000 model runs')\n",
    "plt.xlabel('Vessel Numbers')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(si)\n",
    "res.rename(index={0:'price',1:'cost per unit effort'},inplace=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S1 is the ratio of the variance of vessel days conditional on a particular parameter value to the variance of vessel days. It is the first-order Sobol sensitivity index. The results indicate that cost per hook has a far greater impact on the sensitivity of the number of vessels than does the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
